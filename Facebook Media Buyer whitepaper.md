

# Full-Stack Autonomous AI Media Buyer Agent for Facebook Ads

## Abstract

In this work, we introduce a full-stack **autonomous AI media buyer agent** designed to function as a single "AI employee" managing Facebook advertising campaigns end-to-end. The goal is to automate the role of a performance marketer: the agent autonomously launches and optimizes ads, evaluates creative effectiveness, proposes new advertising angles and funnel (landing page) ideas, and seamlessly communicates with human collaborators for approvals and inputs. The system integrates a large language model (LLM) with long-term memory, a relational database for performance data, and a knowledge graph to connect key marketing concepts. Through iterative testing and learning cycles, the agent analyzes ad performance data (e.g. click-through rates, conversions, ROI) and uses self-improvement logic grounded in copywriting and visual psychology research to generate better creatives and campaign strategies. Human feedback is incorporated via an Asana task loop for tasks like landing page creation and compliance review. We demonstrate how this autonomous agent can adapt and optimize campaigns much like a human media buyer, continually learning from data and interactions. This **AI media buyer** showcases a novel approach to performance advertising, highlighting the potential for a single-agent marketing agency powered entirely by AI.

## Introduction

Modern performance advertising on platforms like Facebook has become increasingly complex and data-driven. Media buyers today must rapidly test numerous ad creatives, target audiences, and landing page funnels to find what drives conversions. This process is labor-intensive and requires balancing creative intuition with rigorous data analysis. As advertising algorithms evolve and competition increases, marketers are challenged to make **faster decisions** and manage larger portfolios of ads than ever before. At the same time, recent advances in artificial intelligence—especially large language models and automation frameworks—offer new opportunities to streamline and even fully automate these tasks.

In this context, we propose an autonomous AI agent that can perform the end-to-end role of a media buying specialist. The vision is an "AI employee" that can **orchestrate Facebook ad campaigns from start to finish** without continuous manual intervention. This agent leverages AI to: generate and launch new ad creatives, monitor their performance, learn which marketing angles or messages resonate, suggest improvements to landing pages and the user funnel, and interface with human team members when needed (for creative approvals, new asset requests, etc.). By integrating advanced reasoning capabilities with programmatic access to advertising platforms, such an agent addresses the need for agility and scale in modern digital marketing.

Recent developments in AI agents (e.g. AutoGPT-like systems) and platform APIs make this feasible. Our approach builds on this by incorporating long-term memory and knowledge integration so the agent can accumulate learnings over time. The **novelty** of our system lies in combining multiple components – a relational database, a knowledge graph memory, and external service integrations – to create a holistic autonomous marketing agent. In the following sections, we detail the architecture and components of this AI media buyer, covering how it evaluates creatives, models economic outcomes, conducts autonomous ad testing, collaborates with humans, and continually improves. We also discuss the implications of treating an AI as a full-fledged member of a marketing team and outline future enhancements to further approach the capabilities of a *single-employee AI marketing agency*.

## System Architecture

The autonomous media buyer agent is built as a **full-stack system** that integrates a language-model-based reasoning core with persistent memory and data storage, as well as connections to external APIs (Facebook Ads, Asana, etc.). The architecture (illustrated conceptually in Figure 1) consists of several layers and components working together:

- **LLM Agent Core:** At the heart is a large language model (such as OpenAI's GPT-4) that serves as the reasoning engine. This LLM is prompted with the agent’s objectives and current context, enabling it to make decisions like selecting which new ad to test or how to respond to performance data. The agent runs in iterative cycles, where it observes the state (recent ad metrics, knowledge base, pending tasks), then reasons and outputs an action (e.g. launch a new ad variant, or create a task for a human collaborator).

- **Memory and Knowledge Integration (Zep + Graphiti):** To give the agent long-term memory and the ability to handle both structured and unstructured information, we use **Zep**, a memory platform that builds a dynamic knowledge graph from the agent’s interactions and data ([Zep - AI Agent](https://aiagentstore.ai/ai-agent/zep#:~:text=Zep%20is%20an%20AI%20memory,aware%20AI%20applications)). Zep’s core component, *Graphiti*, is a temporally-aware knowledge graph engine that combines unstructured conversational data with structured business data ([[2501.13956] Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956#:~:text=enterprise%20applications%20demand%20dynamic%20knowledge,that%20dynamically%20synthesizes%20both%20unstructured)). In our system, Graphiti ingests the dialogue-like reasoning traces of the agent (conversations with itself or with humans) as well as structured marketing data (performance metrics, campaign facts from the database). This memory layer allows the agent to recall relevant facts from past advertising experiments without needing to explicitly query all data every time. It maintains historical relationships between entities (ads, dates, results, etc.), enabling temporal reasoning such as identifying trends or remembering past decisions. By unifying diverse data sources in a knowledge graph, the agent can retrieve context dynamically and reason about state changes over time, addressing a key limitation of static context windows ([[2501.13956] Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956#:~:text=enterprise%20applications%20demand%20dynamic%20knowledge,further%20validated%20through%20the%20more)).

- **Relational Database (PostgreSQL):** Alongside the knowledge graph, we maintain a **PostgreSQL** database that serves as the system’s structured data warehouse. The database stores detailed performance metrics (impressions, clicks, spend, conversions, revenue) on a per-ad or per-day basis, as well as metadata about creatives, campaigns, and experiments. While the knowledge graph captures relationships and summary knowledge, the relational DB provides accurate numerical data and time-series records for computations. For example, the agent can query the DB for the past 7 days of spend and revenue for a particular ad to calculate its return on investment. The integration between PostgreSQL and the Graphiti knowledge graph is crucial: the agent periodically syncs data between them. New ads and their metadata are inserted into the DB and also reflected as nodes/edges in the graph; performance updates from the DB are used to annotate the graph with outcomes. This dual storage ensures the agent benefits from **both** precise analytics and high-level relational reasoning.

- **External Integrations:** The agent interacts with external systems through specific integration modules:
  - **Facebook Marketing API:** Using Facebook’s Graph API and Marketing API, the agent can programmatically create and manage ads. It launches new ad campaigns or ad sets in a dedicated Facebook Ads account designated for testing. Through the API, it sets budget, targeting, uploads creative assets (or selects existing ones), and monitors performance. This direct integration allows the agent to execute its decisions (e.g., to run a new ad variant) without human intervention, treating the Facebook Ads platform as an execution environment for its strategies.
  - **Asana API (Human Collaboration):** The agent creates tasks in Asana to loop in human team members when needed. For instance, if the agent determines a new landing page is needed or a new angle requires fresh image assets, it will use the Asana API to create a task in a project board (e.g., a task like "Create a landing page highlighting _Feature X_" assigned to a web designer). The agent can later check Asana for task updates or completion and retrieve any provided information (like the URL of the new landing page).
  - **Cloud Storage (Google Drive/Sheets):** For certain hand-off points such as compliance review of creative assets, the agent may output content to a shared medium. In our implementation, the agent writes proposed ad copy and image references into a CSV file on Google Drive. The compliance team or creative director can review this file to approve or edit content. This is facilitated by either direct API calls to Google Sheets or simply by having a predetermined location the humans know to check. Once reviewed, the agent can be informed (via Asana task comments or a new file) which suggestions are approved.

These components work in concert as follows. The **LLM core** is orchestrated with a loop that can access tools (database queries, API calls, etc.) as needed. Each cycle, the agent pulls recent performance data from PostgreSQL, queries the knowledge graph for context (e.g. similar past experiments or known best practices relevant to the current scenario), and then decides on next actions. The agent’s chain-of-thought and decisions are stored via Zep so that each cycle’s experiences inform the next. The **architecture’s design** ensures that the agent has a memory of past interactions and results (via Zep/Graphiti) and a reliable source of truth for metrics (Postgres), and can take real-world actions (via external APIs). 

**Figure 1: System Architecture Overview.** *The AI media buyer agent (center) uses an LLM for reasoning, augmented by long-term memory (Zep’s knowledge graph) and a performance database (PostgreSQL). It interacts with Facebook Ads via API to run experiments, and with human collaborators via Asana and shared files. Ad creatives, angles, landing pages, and performance metrics are interconnected through the knowledge graph, enabling the agent to reason about which strategies work best.*【❌†embed_image】 *(Illustration of system components and their interactions.)*

## Creative and Funnel Intelligence Module

A core responsibility of the media buyer agent is to intelligently generate and evaluate advertising **creatives** (the ad copy text, images or videos, and overall messaging angle) and to suggest improvements to the **funnel** (the landing pages and user journey after the ad click). This module of the agent combines data-driven analysis with creative strategy principles:

**Learning from Ad Performance:** The agent continuously analyzes how each ad creative is performing. Key metrics include click-through rate (CTR), conversion rate (CVR) on the landing page, cost per acquisition (CPA), and user engagement metrics (likes, comments, shares on the ad). By comparing these metrics across different ads, the agent can infer which creative elements are resonating with the audience. For example, it might observe that ads featuring a certain *angle* (“money-saving” theme in the copy) have a 30% higher CTR than those focusing on another angle, or that ads with images of people using the product lead to more conversions than product-only images. These insights are stored in the knowledge graph for long-term reference (e.g., a node for the "Money-Saving Angle" would be linked to creatives that used it and their performance outcomes).

**Self-Improvement Logic:** Using the above performance insights, the agent employs self-improvement algorithms to propose new creatives. This logic is informed by research in copywriting and visual psychology. The agent has access to a knowledge base of effective advertising principles – for instance, it knows about techniques like **social proof**, **urgency/scarcity**, emotional triggers, and design cues that attract attention. When a particular ad underperforms, the agent reflects on *why* it might have failed (using both data and these principles). It might conclude, for example, that an ad's headline did not create enough urgency or that the image might not have stood out in the news feed. The agent then generates new variations that apply appropriate improvements (e.g., adding an explicit limited-time offer to the headline, or switching to a more vibrant image). If certain colors or imagery have historically worked better (perhaps data shows ads with warm colors had higher engagement), the agent will incorporate those findings. Over time, this creates a feedback loop where each new generation of ads is informed by the success/failure of previous ones.

**Generative Creative Production:** Leveraging the LLM, the agent can generate **ad copy ideas** and even concept suggestions for images. For copy, the model can be prompted to produce multiple headline variations or body text options highlighting different benefits. These are filtered by the agent using both rules (to ensure compliance with platform policies or brand tone) and by estimated performance (the agent might predict which copy aligns with known high-performing language). Notably, recent studies have shown that AI-generated taglines and copy can approach the effectiveness of original human-written versions when guided correctly ([
        Test for the Best: Using ChatGPT to Create Effective Ad Taglines
      -  Experts@Minnesota](https://experts.umn.edu/en/publications/test-for-the-best-using-chatgpt-to-create-effective-ad-taglines#:~:text=Marketers%20can%20leverage%20a%20,class%20taglines%20with%20minimal)). Our agent builds on this: it generates a diverse set of creatives and then selects the most promising for testing, essentially doing an automated "test for the best" as suggested in advertising research ([
        Test for the Best: Using ChatGPT to Create Effective Ad Taglines
      -  Experts@Minnesota](https://experts.umn.edu/en/publications/test-for-the-best-using-chatgpt-to-create-effective-ad-taglines#:~:text=purchase%20intent,class%20taglines%20with%20minimal)). For images, if integrated with generative image models or design templates, the agent could create mockups (e.g., using DALL·E or stable diffusion for concept art), but in our current setup it typically suggests ideas (like "try an image of a happy family using the product outdoors") which a human designer or a library search can later fulfill. This approach ensures the agent stays within creative guidelines and leverages human polish where needed.

**Funnel and Landing Page Analysis:** Beyond the ad itself, the **landing page** is critical to conversion. The agent monitors metrics like bounce rate or conversion rate on each landing page variant (which can be obtained if analytic tools or pixel data are integrated via the database). Using its LLM capabilities, the agent can also **review the content of landing pages**: for instance, it may scrape the text of the page or use stored descriptions. It checks for consistency between the ad and the page (if the ad promises "50% off", is that reflected prominently on the page?) and for adherence to known best practices (clear call-to-action, trust badges, etc.). If performance is lagging, the agent will propose hypotheses for improvement, such as *"The landing page might be too slow or not mobile-optimized"* or *"We should add a testimonial section to reinforce trust, since the ad angle emphasizes reliability."* These suggestions draw on conversion rate optimization (CRO) heuristics.

**Creative Testing Strategy:** The Creative and Funnel Intelligence Module doesn't just create ideas in a vacuum; it also plans how to test them. The agent prioritizes which new creatives or page changes to test based on potential impact. For example, if the agent identified a promising new angle from prior data (say a lot of users responded well to an "eco-friendly" message in one ad), it might generate a set of new ads all exploring that angle in different ways for a thorough test. Conversely, for funnels, it might set up an A/B test between two landing page versions if it suspects a particular page element is causing drop-off. All these test plans are logged for memory, so the agent remembers what it is currently testing and why, which will later be correlated with results.

In summary, the Creative and Funnel Intelligence Module gives the agent a **sense of strategy and creativity**: it learns what works, comes up with new ad creatives and funnel tweaks inspired by both data and marketing psychology, and prepares these ideas for real-world testing. This continuous creative ideation cycle is central to the agent’s ability to improve campaign performance autonomously over time.

## Economic Portfolio Modeling

Launching and scaling ads inherently involves financial trade-offs. Our autonomous agent treats the collection of active ads and tests as an **investment portfolio**, where each ad or idea is an asset with an uncertain return. The system’s backend includes an Economic Portfolio Modeling component to calculate profitability and guide the agent’s budget allocation and testing decisions.

**Profitability Metrics (7-day and 30-day Payout vs CPA):** In many advertising scenarios (especially in performance marketing or affiliate marketing), the revenue from a converted customer is realized over time. For instance, there might be an initial payout or sale value within the first week, and additional revenue (repeat purchases, subscriptions, upsells) by day 30. The agent continuously computes the **7-day and 30-day payout** for each ad or campaign – essentially the total revenue attributable to that ad within 7 days of a user clicking, and within 30 days, respectively. These figures are compared against the **cost per acquisition (CPA)** or ad spend per conversion over the same periods. If an ad’s 7-day revenue exceeds its CPA, it is immediately profitable in the short term; if not, the agent looks to the 30-day payout to see if the ad is likely to break even in a longer window. This mirrors how human marketers evaluate customer lifetime value (LTV) against acquisition costs. By having both short-term and medium-term ROI calculated, the agent can categorize experiments: perhaps some ads are “slow burners” that lose money in week 1 but become profitable by week 4, whereas others either succeed or fail quickly.

These calculations are powered by time-series data stored in PostgreSQL. The agent might run a SQL query to gather the cumulative conversions and revenue for each ad ID in the last 7 and 30 days, as well as the spend. With this data, it computes metrics like **Return on Ad Spend (ROAS)** and profit margin. The results are then stored back into the knowledge graph (as attributes on the creative or campaign nodes) so they can be referenced during reasoning (e.g., a node might contain "ROAS_7d = 0.8, ROAS_30d = 1.3" indicating short-term loss, long-term profit).

**Time-Series Analysis and Trend Projection:** Beyond raw calculations, the agent uses basic time-series analysis to inform decisions. For example, it looks at the trajectory of an ad’s performance: if costs are steady but 7-day revenue is trending up over the last few days, it might infer that the ad’s performance is improving as the Facebook algorithm optimizes delivery (or as word-of-mouth spreads). Conversely, if an initially promising ad’s performance is decaying (declining conversion rates or rising CPAs), the agent notes that trend. This temporal awareness helps decide whether to keep an ad running, pause it, or increase its budget. We use simple forecasting techniques like moving averages and exponential smoothing on the performance metrics to detect trends. More advanced implementations could integrate an AutoML time-series model to predict future ROI, but even straightforward trend logic provides guidance for the agent’s decisions on scaling or sunsetting ads.

**Explore-Exploit Budget Allocation:** Managing a portfolio of ads requires balancing **exploration versus exploitation** – a well-known dilemma in both reinforcement learning and marketing. The agent employs a strategy akin to a multi-armed bandit approach ([Customer Acquisition via Display Advertising Using Multi-Armed Bandit Experiments](https://deepblue.lib.umich.edu/handle/2027.42/102281#:~:text=adapt%20to%20intermediate%20results%20and,armed%20bandit%20%28MAB%29%20methods)). As it tests multiple ads (the "arms"), it reallocates budget towards those showing better performance while still allocating some budget to new tests. For instance, suppose the agent has 5 active test ads. After an initial period, 2 of those show strong CPA vs payout numbers (profitable or very high engagement), 2 show mediocre results, and 1 is clearly underperforming. The agent will *exploit* the winners by increasing their daily spend or impressions allocation (e.g., moving budget from the general "research" pool into these ads), aiming to maximize returns from what is working. Simultaneously, it *explores* by launching further variations or new angles to continue searching for even better performers. Under the hood, the agent’s logic for this might implement rules similar to **Thompson Sampling or Upper Confidence Bound (UCB)** approaches from multi-armed bandit algorithms to decide how to distribute the budget optimally. This kind of adaptive experimentation has been shown to improve marketing outcomes by finding winning creatives faster and allocating impressions more efficiently ([Customer Acquisition via Display Advertising Using Multi-Armed Bandit Experiments](https://deepblue.lib.umich.edu/handle/2027.42/102281#:~:text=campaign%20with%20a%20large%20retail,to%20evaluate%20a%20range%20of)) (in one field experiment, a bandit policy improved customer acquisition rate by 8% with no additional spend ([Customer Acquisition via Display Advertising Using Multi-Armed Bandit Experiments](https://deepblue.lib.umich.edu/handle/2027.42/102281#:~:text=campaign%20with%20a%20large%20retail,to%20evaluate%20a%20range%20of))).

**Risk Management and Constraints:** The portfolio model also enforces business rules and risk limits. We cap the total daily "experimental budget" that the agent can spend on new ideas, to ensure it doesn’t overspend in pursuit of exploration. The agent is programmed to consider statistical significance as well – it won’t cut a test too early just because of random noise, and conversely, it won’t chase a false positive. It uses heuristics like requiring a minimum number of impressions or conversions before trusting performance metrics. If an idea is flagged as a probable failure (e.g., zero conversions after a certain spend), the agent will stop the test to conserve funds. Essentially, the agent behaves like a portfolio manager curating a set of ad investments: constantly measuring **ROI**, doubling down on winners, trimming losers, and keeping an eye on the overall budget limits and goals (such as a target cost-per-acquisition).

The Economic Portfolio Modeling component thus grounds the agent’s decisions in quantitative viability. This ensures that creative ideas are not only interesting but also aligned with profitable growth, tying together the *creative experimentation* with *financial outcomes*. By automating these calculations and decisions, the agent can manage far more simultaneous experiments than a human, while objectively steering towards higher ROI for the campaign as a whole.

## Autonomous Testing via Facebook Ads

To validate the ideas generated by its creative and portfolio reasoning, the AI agent conducts **autonomous ad testing** on Facebook. We utilize a dedicated Facebook Ads account (a "sandbox" research account) with a modest budget allocated for continuous experiments. The agent’s operation in this space can be thought of as an ongoing cycle of hypothesize → test → learn, implemented through the Facebook Marketing API.

**Test Campaign Deployment:** When the agent decides on a new ad or a variation to test, it uses the Facebook Marketing API to create the necessary campaign structure. Concretely, it will create a new Campaign (if needed) or an Ad Set under an existing campaign designated for testing, and then create a new Ad within that Ad Set. Targeting parameters are kept broad or use a predefined research audience, unless the test is specifically about audience segmentation. The budget for each test Ad or Ad Set is set small initially (for example, $10-$50 per day) to limit risk. The agent provides the creative elements: it uploads the image or video asset (or selects one by ID if it's already in the library), and submits the ad copy text and headline generated in the previous steps. It also sets the destination URL to the appropriate landing page for the test.

All these operations are performed through API calls without human intervention. The agent logs the new ad’s details in the PostgreSQL database and links it in the knowledge graph to the creative concept it represents (e.g., tying the new Ad node to the "angle" it was meant to test). Each test ad is tagged in the system as an “experiment” along with the hypothesis or reason (recorded in the graph or as a note), so the agent knows later *why* it launched that test and what it was trying to validate.

**Expanding Research Budget and Scale Decisions:** The test account has an overall budget (perhaps a few thousand dollars per month) that the agent manages. If an experiment starts to show positive results (e.g., significantly lower CPA than average or a high conversion rate), the agent may **expand the budget** allocated to it. This can be done by increasing the Ad Set budget via the API or moving the ad into a higher-budget campaign. The agent’s policy is to spend more on an experiment as confidence in its success grows – effectively scaling promising tests. In some cases, exceptionally good performers might even be flagged for transfer to a main production ad account (with human approval) once proven. On the flip side, the agent will quickly prune experiments that perform poorly, as identified by the portfolio model.

**Real-time Monitoring and Ad Sentiment Analysis:** Once ads are running, the agent monitors their performance in near real-time by pulling data from the Facebook API. It checks metrics like impressions, clicks, CTR, spend, and conversions at regular intervals (e.g., every few hours) and logs these to the database. Beyond quantitative metrics, the agent also analyzes **ad sentiment** qualitatively. It uses the Facebook Graph API to fetch comments and reactions on the test ads. Using sentiment analysis (an NLP sub-task), the agent gauges how users are reacting. For example, overwhelmingly negative comments or angry reactions might indicate an issue with the ad’s messaging or that it’s being shown to an inappropriate audience segment. The agent flags such cases in the knowledge graph (e.g., tagging the creative as having “negative feedback spike”) and may decide to pause or tweak the ad copy to address the feedback. Positive engagement, such as comments tagging friends or praising the product, is also noted as a good sign. This sentiment feedback loop is important because an ad that gets engagement and positive sentiment can improve relevance score and deliverability on Facebook, whereas negative feedback can hurt performance or risk ad disapproval.

**ROI Feedback Loop:** As the ads run, the **ROI calculations** discussed earlier feed back into the agent’s reasoning loop. The agent might run an overnight analysis job to compute the current 7-day and 30-day ROAS for each active test and then compare them. These results are used at the start of the next cycle to decide which tests to continue. For example, if a new ad achieved 5 sales with $100 spend (maybe $25 revenue each, so $125 total revenue on $100 spend = 1.25 ROAS in the first few days), the agent sees promise and keeps it going. If another ad spent $50 and got 0 conversions, the agent will likely terminate that experiment the next day as a failed test.

Crucially, the autonomy here means the agent can run dozens of tests in parallel, something a human might struggle to manage at granular levels. It is constantly learning from each test. Over time, patterns emerge in the knowledge graph: the agent might observe, for instance, that *“ads testing the ‘free trial’ angle consistently yielded ROAS > 1, whereas those focusing on ‘discount pricing’ struggled”*. These insights inform not only immediate scaling decisions but also the next creative hypotheses to try.

By **automating the test execution** on Facebook and closing the loop with data analysis, the agent effectively operates a perpetual optimization engine. It treats the test ad account as a laboratory where theories about what advertising approaches work are proven or disproven with real audience responses. This capability is what allows the system to move fast—far faster than a traditional cycle where a human devises a campaign, waits a week or two for results, then manually analyzes and adjusts. Here the turnaround from idea to live test to result can be a matter of a day or even hours.

## Human Collaboration Loop

While the agent strives for end-to-end autonomy, human collaboration remains an essential part of the workflow for tasks that require creative production, oversight, or decisions beyond the agent’s current scope. The system is designed to integrate a **human-in-the-loop** at key junctures via existing project management and collaboration tools, ensuring that the AI works *with* humans as a co-worker rather than a black-box executor. The primary mechanism for this is an integration with **Asana**, a project management platform, and supplemental use of Google Drive for asset sharing.

**Task Creation in Asana:** When the agent encounters a task it cannot complete alone or requires human approval, it creates an Asana task in a designated project (for example, an “AI Media Buyer Requests” board). Each task created by the agent includes a clear description of what is needed, why it's needed, and any relevant context. For instance:
- If the agent determines a new **landing page** variant is needed (maybe to support a new angle that current pages don’t address), it will create a task like: *"Create a new landing page focused on **security features** – emphasize trust and security in design and copy. (Needed because our new ad angle “secure solution” is performing well, but current LPs don’t mention security.)"* The task might be assigned to a web developer or marketing team member. The agent will include details such as suggested headlines or content it thinks should be on the page, based on its reasoning.
- For **creative asset requests**, say the agent wants an image of a specific concept that it cannot generate itself, it will create a task: *"Design an image for Ad **ID123**: showing a family using the product outdoors (angle: ease-of-use in everyday life). Needed for new ad test."* This might be assigned to a graphic designer, and the agent can attach a rough AI-generated image or a storyboard as a starting reference if available.

The agent sets due dates or priority on tasks depending on urgency (e.g., landing page needed before scaling a campaign might be high priority). It also tags tasks with identifiers so it can match the responses later (for example, including the ad ID or idea name in the task title for correlation).

**Receiving Human Input:** Through Asana’s API or by monitoring the task status, the agent waits for resolution. Once a human marks the task as complete (or adds a comment with the requested information), the agent will fetch the details. In the landing page example, the developer might comment with the URL of the new page or attach screenshots. The agent then takes that URL, updates its database (adding a new LandingPage entry, linking it in the graph to the respective angle and creative), and can start directing traffic to it in the next round of tests. In the image request example, the designer might upload the new image to a shared drive and paste the link in Asana. The agent will retrieve the asset (perhaps download it or note its ID if it's directly added to the ad account library) and then proceed to create the ad using it.

**Compliance and Approval Workflow:** Advertising in certain domains requires strict compliance checks (to avoid disallowed content or claims). Also, companies often want a brand voice check on AI-generated copy. To facilitate this, the agent generates **batch requests for review**. It compiles a CSV (spreadsheet) on Google Drive listing new ad copy suggestions and any image thumbnails or references, and then pings the compliance or marketing lead (possibly via Asana or email) to review. For example, the CSV might have columns: "Proposed Headline", "Proposed Text", "Image description", "Reason/Angle". A human reviewer can go through these entries, make edits if necessary, or mark approved/rejected. The agent, upon being notified (or at a scheduled interval), reads this CSV back. Using the Google Drive API or Sheets API, it can see which entries are approved and use only those in subsequent deployments. This approach ensures a **human veto** power over anything the agent creates, which is important in the early stages of trust-building. It is analogous to a junior employee sending work to a senior for sign-off. As the agent demonstrates reliability, this process can be streamlined, but it remains in place as a safety net.

**Collaboration on Experiment Ideas:** The agent also accepts **human-initiated requests** via Asana. Team members can create tasks for the AI agent, for instance: *"AI Agent: evaluate why Campaign X’s performance dropped last week"* or *"AI Agent: brainstorm new ad ideas for the holiday promotion."* Through a combination of custom Asana tags and a scheduling loop, the agent will pick up these tasks, treat the description as a prompt, and then produce an analysis or list of ideas, which it then comments back into the task or attaches as a document. In this way, humans can leverage the agent’s analytical and generative abilities on demand. This two-way interaction builds a collaborative environment where the AI and humans each focus on what they do best (the AI crunches data and suggests, the humans provide final creativity and oversight).

The **human collaboration loop** is critical for integrating the AI agent into a real organizational workflow. It ensures transparency (the team can see what the agent is working on via tasks), control (humans can intervene or redirect as needed), and learning (the agent incorporates feedback it gets from humans into its memory for future). This design echoes the human-in-the-loop review process found in other AI marketing systems, where an AI works autonomously but final content alignment is ensured by human review ([Best AI Agents for Digital Marketing: The Ultimate Guide for 2025](https://digitalagencynetwork.com/best-ai-agents-for-digital-marketing/#:~:text=Skott%20works%20autonomously%2C%20but%20you%E2%80%99re,perfectly%20with%20your%20brand%E2%80%99s%20standards)). Over time, as trust in the agent grows, the humans might give it more autonomy (perhaps auto-approving trivial changes), but maintaining this collaborative dynamic is important for accountability and compliance.

## Graph Data Model

At the heart of the agent’s reasoning capabilities is a rich **graph data model** that represents the relationships between all key elements of the advertising domain. We use Aura Graphiti (the graph knowledge component of Zep) backed by a Neo4j graph database to store and query this information. The graph approach enables the agent to perform complex queries like “find all ad creatives that used a similar angle and had high conversions” or “trace which landing page each conversion came from” in an intuitive way.

**Node Types:** The graph consists of several primary node types:
- **Creative:** Represents an individual ad creative variant. Each Creative node has attributes like an ID, the ad copy text, the image or video reference, and maybe the date it was launched. Creative nodes are central in the graph as they link to angles, campaigns, and results.
- **Angle:** Represents a marketing angle or theme. For example, "Budget-Saving Angle", "High-Quality Angle", "Emotional - Family Oriented". These are abstract nodes that categorize creatives. An Angle node might have a short description of the approach (e.g., *"Emphasizes saving money for the user"*) and perhaps links to any research or past notes about this angle’s effectiveness.
- **LandingPage (LP):** Represents a landing page or funnel step (like a quiz page, sign-up form, etc.). An LP node holds the URL and perhaps metadata like which product or offer it is for.
- **ConversionEvent:** Represents a conversion action (purchase, signup, etc.) and/or aggregated results. We model conversions in two ways: (a) We have nodes for individual conversion events or batches (like "Conversion on 2025-04-01 from Ad X on LP Y with revenue $Z"), or (b) we attach conversion counts as properties on relationships (more on that next). Individual event nodes can be useful for analysis, but can also be numerous, so in practice we often store summary stats as properties.

**Relationships:**
- **Creative –HAS_ANGLE→ Angle:** This relationship links an ad to the angle it embodies. (If an ad tries to use multiple angles, it could have multiple of these relationships, but usually there’s one primary angle.) This allows the agent to find all creatives under a given strategy theme.
- **Creative –POINTS_TO→ LandingPage:** This denotes which landing page the ad directs traffic to. If an ad was tested with multiple landing pages (say via split testing), it could have multiple such relationships, or we can model that as multiple Creative entries for each combination. Typically, though, one ad corresponds to one URL.
- **LandingPage –CONVERTS→ ConversionEvent:** This can link a landing page to conversion events that occurred on it. For example, if the conversion is a purchase, the ConversionEvent node might contain the revenue or item details, and it would be connected to the LP where it happened. If conversion tracking is precise, we might further link the ConversionEvent back to the Creative that led the user (Creative –→ ConversionEvent) to capture the full funnel path.
- **Angle –INFLUENCED→ ConversionEvent:** We might also link angle nodes to conversions in an aggregate sense, to see which marketing angles ultimately drove the most conversions (through any creative that carried that angle). This can be derived via Creative, but having a direct relationship can simplify queries like "total conversions from angle X".

Additionally, there could be relationships like **Creative –TESTED_IN→ Campaign/AdSet** if we model the campaign structure, or **Angle –RELATED_TO→ Angle** if we want to cluster similar angles. In our case, the main focus is the creative-angle-LP-conversion linkage.

**Properties and Temporal Data:** Graphiti being temporally-aware means we can store time-stamped information. Rather than naively updating a node’s property (losing history), Graphiti can maintain historical properties. For instance, a Creative node might have a property "CTR" that changes over time – Graphiti can keep a timeline of CTR values. Or we might keep separate nodes for “Creative Performance at Week 1” etc., but that's less elegant. We also use the graph for storing summary metrics: e.g., the relationship Creative –POINTS_TO→ LP might have properties like `conversions_7d=10, conversions_30d=20` and `revenue_7d=$500`, etc., representing how that creative→LP pairing performed. This way, a single query on the graph can retrieve not just structure but key numbers.

**Example:** To illustrate, suppose we have a Creative (Ad ID 101) which is an ad that says "Save $500 with our solution" and shows a piggy bank image. We link Creative101 –HAS_ANGLE→ Angle("Budget-Saving"). Creative101 –POINTS_TO→ LandingPage("PricingPromoPage"). If 3 users converted after clicking this ad, we might create 3 ConversionEvent nodes linked from that landing page (or one node with count 3). Each ConversionEvent could link back to Creative101 (or we just note Creative in a property). In the knowledge graph, we could then query: “find all Angles that have ConversionEvents with revenue > X” to see top themes, or “for a given Angle, find all LPs that it led to and the sum of conversions”.

**Why Graph Model:** This connected structure is incredibly useful for the agent’s reasoning. For example, if the agent wants to know why a certain angle isn’t working, it can traverse: Angle → Creative → ConversionEvent to see the outcomes, then Angle → Creative → (maybe other metrics). If it wants to find a new angle, it could look at Angle nodes that have few creatives tested and perhaps cross them with external knowledge (Graphiti allows incorporation of unstructured data too, so we could link an Angle node to, say, some market research text or a conversation snippet where a human suggested that angle). The graph also makes it easier to detect **patterns**: e.g., maybe a particular LandingPage is used by many creatives but only those with a specific angle convert well – that insight (a LandingPage node connected to multiple creatives but conversions only on the ones also linked to Angle X) can emerge from graph queries.

**Aura and Performance:** We utilize Neo4j AuraDB (a cloud-managed Neo4j database) for reliability and ease of maintenance. The Graphiti library interfaces with Neo4j to store and retrieve data. Queries are made either via Cypher queries written by the agent (the agent can generate Cypher queries using the LLM, guided by templates for common queries) or via Graphiti’s higher-level search functions. The agent might ask a question in its internal reasoning like: *"Have we tested any angle similar to 'security' that had good ROI?"* – the system can translate that to a graph query that looks for Angles related to security, finds connected creatives and their ROI property, then returns an answer. This is far more natural than trying to join multiple SQL tables and piece it together in code, showcasing the benefit of a graph for relational knowledge.

In summary, the graph data model provides a **structured knowledge backbone** for the AI agent. It captures the who-what-where-why of the advertising efforts: which *Creatives* (who) use which *Angles* (what theme), point to which *Landing Pages* (where the user goes), and produce which *Conversions* (why it matters, in terms of results). This rich interconnected dataset, kept updated in near real-time, is a key enabler for the agent’s sophisticated decision-making.

## Memory and Reflection via Zep

Beyond the immediate data of ads and performance, our AI agent benefits from an overarching memory and reflection mechanism powered by Zep’s long-term memory store. This allows the agent to **learn from experience**, not just react to the latest data. While the graph model provides a snapshot of knowledge, the *Memory and Reflection* component enables the agent to maintain context over extended periods and to improve its reasoning strategies over time.

**Episode Memory Storage:** Each cycle of the agent’s operation (for example, a daily run or a self-contained decision process) can be considered an *episode*. During that cycle, the agent might engage in an internal dialogue (chain-of-thought prompt) and possibly some interactions with humans (like asking for an asset) or observations (like results coming in). All these unstructured elements – the prompts, the generated thoughts, the human chat excerpts, any intermediate conclusions – are stored by Zep. Specifically, Zep captures these as part of its knowledge graph (Graphiti) with temporal context. For instance, the agent might have a memory entry: *"2025-04-05: Noticed that angle 'X' failed in two campaigns. I hypothesize it's due to seasonality."* or *"2025-04-10: Compliance feedback: avoid using the word 'guarantee' in copy."* These are stored as nodes or notes linked by timestamp and relevant entities (angle X node, compliance rule node). Over time, this becomes a rich timeline of the project.

Storing conversations and reasoning traces means the agent doesn’t forget why certain decisions were made. If a similar situation arises, it can retrieve the prior reasoning. For example, if months later the agent again considers using angle X, it can query memory: *"What happened last time we tried angle X?"* and Zep/Graphiti can surface the note that it failed and the hypothesized reason. This prevents the agent from repeating mistakes or at least prompts it to adjust approach (maybe it will try angle X again but in a different season, or with a tweak, if it thinks conditions changed).

**Reflection Process:** The agent periodically engages in a reflection step, which is a deliberate analysis of past events to improve future performance. Using the long-term memory, the agent might summarize the past week’s experiments in a narrative form and analyze it. This could be prompted as: *"Summarize this week's campaign outcomes and key lessons."* The LLM will then generate a summary citing specific experiments (drawing from the memory store data). This summary is saved, and the agent’s policy module parses it to update any internal strategy parameters. For example, a reflection might conclude: *"Ads with emotional appeal consistently beat feature-focused ads this week. Also, our funnel is leaky on the pricing page."* As a result, the agent might set a rule to prioritize emotional angles for the next cycle and to allocate time to fix the funnel issue (perhaps by creating an Asana task).

This reflective practice is akin to a human marketer’s weekly review meeting, ensuring the agent not only reacts but *learns structurally*. Zep’s memory graph helps in this by allowing cross-session information synthesis – the agent can connect dots that span multiple sessions (something standard LLM usage struggles with). Zep’s approach has been shown to excel at cross-session context maintenance ([[2501.13956] Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956#:~:text=challenging%20LongMemEval%20benchmark%2C%20which%20better,world%20applications)), which we leverage here for complex temporal patterns in advertising (for instance, *"every December, ads with theme Y have worked; recall that next December"* could be captured).

**Use of Memory in Decision Making:** Whenever the agent is about to make a major decision (like allocating a big budget to scale an ad, or completely dropping a strategy), it consults the memory for similar precedents. It does this via semantic search and graph queries in Zep. If the agent is unsure how to handle a situation, it might pose a question to itself like, *"Have I encountered a rapid CPA increase like this before? What did I do and did it work?"* The memory could retrieve a scenario from three months ago when a similar CPA spike happened due to audience saturation, and the agent paused the campaign, which turned out to be the correct move. Informed by that memory, the agent might choose to do the same now. This kind of institutional memory is what distinguishes our approach – the agent is not stateless or short-sighted; it gets **better with time** as it accumulates knowledge.

**Maintenance of Historical Data:** The memory system also ensures we don’t lose historical performance data when the state in the database moves on. For example, after many experiments, older campaigns might be deleted or archived from the main DB to save space or because they’re no longer relevant. However, their essence (what angle, what result) remains in the knowledge graph. The agent can recall “years” of data in abstract form even if detailed logs are purged. This is important in advertising where seasonality and year-over-year patterns matter.

In essence, the Memory and Reflection component turns the agent from a reactive script into a **learning agent**. It mimics the way a seasoned marketing manager accumulates intuition and lessons over a career – except here it’s stored explicitly and can be queried. By combining Zep’s long-term memory graph with periodic reflective analysis, the agent avoids the trap of forgetting hard-won lessons. It’s able to refine its own algorithms (for instance, if reflection shows its threshold for pausing ads is too strict, it may adjust that parameter). This self-improvement loop closes the final gap, making the agent *adaptive* in the long run, not just in immediate response to data.

## Future Work and Limitations

While the current system demonstrates a capable autonomous agent for Facebook Ads, there are several limitations to address and opportunities for future enhancement to approach an even more powerful AI media buyer:

**1. Multi-Modal Analysis and Content Generation:** At present, the agent’s analysis of creative effectiveness relies on data and some basic understanding of text. A next step is to incorporate **multi-modal models** so the agent can *see* and *hear* the creative content. By integrating vision AI (e.g., using a model like CLIP or GPT-4V) the agent could analyze an ad’s image or video frame-by-frame to detect features (presence of people, colors, text overlay, etc.) and correlate those with performance. It could learn, for example, that images with a person increase conversion for one product but not for another. Similarly, if video ads are used, a model could evaluate the first 3 seconds of each video to judge hook strength. On the generation side, using advanced generative models for images (such as DALL·E 3 or Midjourney) and even video, the agent could create initial drafts of visual content. Combined with the human review loop, designers could get AI-generated storyboards to start from. This multi-modal capability would make the agent a true creative director, not just a copy generator.

**2. Direct Funnel Deployment:** Currently, changes to landing pages or funnels involve humans. In future iterations, the agent could integrate with web content management systems (CMS) or landing page builders (like Webflow, WordPress, etc.) via APIs. This would allow the agent to *directly deploy* simple page changes or new A/B tests in the funnel. For instance, the agent could automatically create a duplicate of an existing landing page with a different headline or image to test a hypothesis, using a headless CMS API. Of course, guardrails would be needed to avoid major brand missteps, but template-based changes (like rearranging sections or swapping a hero image) could be done autonomously. This moves closer to a world where the AI not only suggests but actually *implements* funnel optimizations end-to-end.

**3. Generalization to Other Platforms:** The current design is tailored to Facebook Ads. Expanding to a multi-platform media buyer (Google Ads, TikTok, etc.) would be a logical progression. Many principles carry over, but the agent would need integration with other APIs and slight adaptation (for example, Google search ads are text-only and have different constraints; TikTok creatives are video-centric). The knowledge graph could be extended to include platform as an attribute of creatives and an element in decision logic (the agent might learn that what works on Facebook doesn’t on Google, and vice versa). A unified agent that can allocate budget across platforms treating each as different “channels” in the portfolio would be very powerful.

**4. Enhanced Learning Algorithms:** While our current system uses heuristic and bandit approaches, future versions could incorporate more **reinforcement learning (RL)**. An RL agent could theoretically learn an optimal campaign management policy through simulation or real-world trial, treating the entire environment as a Markov Decision Process. However, doing this safely in a live business setting is challenging. Another angle is using Bayesian optimization for selecting creative parameters (like testing systematically variations of an image with slight color differences to find optimum). We see potential in exploring these advanced methods to further automate the discovery of high-performing ads.

**5. Scale and Real-Time Constraints:** As the volume of data grows (many campaigns, many ads, lots of memory entries), scaling the system will be a challenge. Neo4j Aura can handle large graphs, but query performance and cost need monitoring. Similarly, the LLM calls (to GPT-4 or others) are a bottleneck in speed and cost; fine-tuning a smaller model or using an in-house model for parts of the task could cut down reliance on API calls. In a high-frequency ad bidding scenario, decisions might need to be made in minutes, which currently the agent might be too slow for (given it reasons in cycles that might take several minutes). Future work could optimize the agent’s loop or have parallel sub-agents handling different tasks concurrently (one focusing on creative generation while another monitors metrics, etc.).

**6. Trust and Safety:** The autonomous agent must continue to uphold compliance, brand voice, and ethical advertising standards. Future improvements should add more robust checks using AI – e.g., an automated compliance classifier that reviews any content before it goes live (beyond just the human review, as a double-check). There is also the aspect of bias – the agent’s decisions are only as good as the data it sees, so it might inadvertently exploit patterns that are discriminatory (e.g., it might find an audience that yields good ROI but in a way that proxies sensitive attributes). Putting constraints and monitoring in place (perhaps having the agent justify certain targeting or creative decisions for human audit) will be important as we scale.

**7. Continuous Human Oversight and Training:** In the foreseeable future, this AI agent works best as a **force multiplier for humans**, not a complete replacement. We envision more of a *centaur* approach (human + AI hybrid) in practice. Thus, future iterations will refine the user interface for human managers to interact with the agent: dashboards derived from the knowledge graph for visibility, simple commands to adjust agent parameters (like “focus on product A this week”), and learning from the humans (the agent could observe how a human expert makes a decision differently and update its logic). Essentially, treating the deployment of the agent as an ongoing training process where human feedback continually refines its behavior (a form of reinforcement learning from human feedback, RLHF, specific to marketing tasks).

In summary, there are many paths to make the AI media buyer even more **autonomous, intelligent, and integrated**. We have tackled the core challenge of proving that a single agent can run a significant portion of a Facebook advertising operation. The future will involve broadening and deepening its capabilities, always with an eye on maintaining reliability and alignment with business goals. With careful development, we foresee a system that can truly function as a *digital CMO* handling multi-channel campaigns with minimal oversight.

## Conclusion

We presented a full-stack autonomous AI media buyer agent that functions as a one-person marketing agency, specialized for Facebook Ads. Drawing on a combination of an LLM reasoning core, a temporal knowledge graph memory (via Zep and Graphiti), and structured data analytics, the agent can execute the end-to-end cycle of performance advertising: from creative ideation and launch, through real-time optimizations, to learning from results and collaborating with humans for production tasks. This integration of technologies allowed us to automate complex workflows that traditionally require cross-functional human teams.

In implementing this system, several notable achievements come forward. First, the agent maintained a long-term understanding of campaign context and historical knowledge, demonstrating that AI agents can have continuity across sessions in an enterprise setting ([[2501.13956] Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956#:~:text=challenging%20LongMemEval%20benchmark%2C%20which%20better,world%20applications)). Second, by treating marketing experiments as a portfolio to manage, it showed a disciplined, data-driven approach to budget allocation analogous to what a seasoned media buyer or financial analyst might do. Third, the seamless loop with human collaborators via Asana and other tools illustrates a practical blueprint for human-AI teamwork in a business process – the agent proactively seeks human input when needed, and humans can delegate routine analysis or creative drafts to the agent, freeing up time for higher-level strategy.

Our AI media buyer agent is a step towards the vision of an **autonomous marketing organization**, where many operational decisions can be handled by AI with oversight rather than execution by people. The novelty of the system is in how it brings together disparate components (databases, APIs, memory graphs, and an LLM) into a cohesive whole that mirrors the responsibilities of a human media buying team. It underscores how recent advancements in AI (especially in language understanding and knowledge graphs) can be applied to very practical, results-oriented domains like advertising.

There are, of course, areas requiring further validation. We treated one specific use-case and platform; the generalization and long-term performance remain to be tested. Moreover, the **single-agent paradigm** has its limits – real organizations might use multiple specialized agents or a hierarchical agent structure. However, our work lays the groundwork by showing that even a single, well-integrated AI agent can deliver a surprisingly comprehensive set of marketing functions.

In conclusion, the project demonstrates the feasibility of a full-stack autonomous media buyer. This AI agent can be seen as a tireless junior marketer that rapidly learns to become an expert, potentially managing campaigns 24/7 with a combination of creative flair and analytical rigor. As we refine this approach and integrate more capabilities, such agents could become standard co-workers in marketing teams – accelerating experimentation, uncovering insights from data, and handling the heavy lifting of campaign management. The **"AI employee"** in marketing is no longer science fiction, but an emerging reality, and our system is an early example of what such an AI employee, acting as a single-employee marketing agency, can achieve in the realm of Facebook advertising and beyond.